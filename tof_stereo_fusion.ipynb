{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tof stereo fusion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GcR_XPWM43yK",
        "colab_type": "code",
        "outputId": "683c78ee-e728-4ca0-a9cf-7372b69f5144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd '/content/gdrive/My Drive/Tesi/Colab working dir'"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Tesi/Colab working dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X4FlHYHd3_1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "### Generator layers\n",
        "def conv_layer(input, w_size, n_filters, reuse=False, norm='instance', activation='relu', is_training=True, name='conv'):\n",
        "  \"\"\" A w_size x w_size Convolution-ReLU layer with n_filters filters and stride 1\n",
        "  Args:\n",
        "    input: 4D tensor\n",
        "\tw_size: integer, window size of the filter (w_size x w_size)\n",
        "    n_filters: integer, number of filters (output depth)\n",
        "    norm: 'instance' or 'batch' or None\n",
        "    activation: 'relu' or 'tanh'\n",
        "    name: string, e.g. 'conv'\n",
        "    is_training: boolean or BoolTensor\n",
        "    name: string\n",
        "    reuse: boolean\n",
        "  Returns:\n",
        "    4D tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(name, reuse=reuse):\n",
        "    weights = _weights(\"weights\",\n",
        "      shape=[w_size, w_size, input.get_shape()[3], n_filters])\n",
        "    h_w_size = int(w_size/2);\n",
        "    padded = tf.pad(input, [[0,0],[h_w_size,h_w_size],[h_w_size,h_w_size],[0,0]], 'REFLECT')\n",
        "    conv = tf.nn.conv2d(padded, weights,\n",
        "        strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "    normalized = _norm(conv, is_training, norm)\n",
        "\n",
        "    if activation == 'relu':\n",
        "      output = tf.nn.relu(normalized)\n",
        "    if activation == 'tanh':\n",
        "      output = tf.nn.tanh(normalized)\n",
        "    return output\n",
        "\n",
        "def last_conv_layer(input, w_size, n_filters, reuse=False, name='conv'):\n",
        "  \"\"\" A w_size x w_size Convolution layer with n_filters filters and stride 1\n",
        "  Args:\n",
        "    input: 4D tensor\n",
        "\tw_size: integer, window size of the filter (w_size x w_size)\n",
        "    n_filters: integer, number of filters (output depth)\n",
        "    name: string, e.g. 'conv'\n",
        "    name: string\n",
        "    reuse: boolean\n",
        "  Returns:\n",
        "    4D tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(name, reuse=reuse):\n",
        "    weights = _weights(\"weights\",\n",
        "      shape=[w_size, w_size, input.get_shape()[3], n_filters])\n",
        "    h_w_size = int(w_size/2);\n",
        "    padded = tf.pad(input, [[0,0],[h_w_size,h_w_size],[h_w_size,h_w_size],[0,0]], 'REFLECT')\n",
        "    output = tf.nn.conv2d(padded, weights,\n",
        "        strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def t_conv_layer(input, w_size, n_filters, reuse=False, norm='instance', is_training=True, name=None, output_size=None):\n",
        "  \"\"\" A w_size x w_size fractional-strided-Convolution-BatchNorm-ReLU layer\n",
        "      with n_filters filters, stride 1/2\n",
        "  Args:\n",
        "    input: 4D tensor\n",
        "    w_size: integer, window size of the filter (w_size x w_size)\n",
        "    n_filters: integer, number of filters (output depth)\n",
        "    norm: 'instance' or 'batch' or None\n",
        "    is_training: boolean or BoolTensor\n",
        "    reuse: boolean\n",
        "    name: string, e.g. 't_conv'\n",
        "    output_size: integer, desired output size of layer\n",
        "  Returns:\n",
        "    4D tensor\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(name, reuse=reuse):\n",
        "    input_shape = input.get_shape().as_list()\n",
        "\n",
        "    weights = _weights(\"weights\",\n",
        "      shape=[w_size, w_size, n_filters, input_shape[3]])\n",
        "\n",
        "    if not output_size:\n",
        "      output_size = input_shape[1]*2\n",
        "    output_shape = [input_shape[0], output_size, output_size, n_filters]\n",
        "    fsconv = tf.nn.conv2d_transpose(input, weights,\n",
        "        output_shape=output_shape,\n",
        "        strides=[1, 2, 2, 1], padding='SAME')\n",
        "    normalized = _norm(fsconv, is_training, norm)\n",
        "    output = tf.nn.relu(normalized)\n",
        "    return output\n",
        "\n",
        "\n",
        "### Helpers\n",
        "def _weights(name, shape, mean=0.0, stddev=0.02):\n",
        "  \"\"\" Helper to create an initialized Variable\n",
        "  Args:\n",
        "    name: name of the variable\n",
        "    shape: list of ints\n",
        "    mean: mean of a Gaussian\n",
        "    stddev: standard deviation of a Gaussian\n",
        "  Returns:\n",
        "    A trainable variable\n",
        "  \"\"\"\n",
        "  var = tf.get_variable(\n",
        "    name, shape,\n",
        "    initializer=tf.random_normal_initializer(\n",
        "      mean=mean, stddev=stddev, dtype=tf.float32))\n",
        "  return var\n",
        "\n",
        "def _biases(name, shape, constant=0.0):\n",
        "  \"\"\" Helper to create an initialized Bias with constant\n",
        "  \"\"\"\n",
        "  return tf.get_variable(name, shape,\n",
        "            initializer=tf.constant_initializer(constant))\n",
        "\n",
        "def _leaky_relu(input, slope):\n",
        "  return tf.maximum(slope*input, input)\n",
        "\n",
        "def _norm(input, is_training, norm='instance'):\n",
        "  \"\"\" Use Instance Normalization or Batch Normalization or None\n",
        "  \"\"\"\n",
        "  if norm == 'instance':\n",
        "    return _instance_norm(input)\n",
        "  elif norm == 'batch':\n",
        "    return _batch_norm(input, is_training)\n",
        "  else:\n",
        "    return input\n",
        "\n",
        "def _batch_norm(input, is_training):\n",
        "  \"\"\" Batch Normalization\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(\"batch_norm\"):\n",
        "    return tf.contrib.layers.batch_norm(input,\n",
        "                                        decay=0.9,\n",
        "                                        scale=True,\n",
        "                                        updates_collections=None,\n",
        "                                        is_training=is_training)\n",
        "\n",
        "def _instance_norm(input):\n",
        "  \"\"\" Instance Normalization\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(\"instance_norm\"):\n",
        "    depth = input.get_shape()[3]\n",
        "    scale = _weights(\"scale\", [depth], mean=1.0)\n",
        "    offset = _biases(\"offset\", [depth])\n",
        "    mean, variance = tf.nn.moments(input, axes=[1,2], keep_dims=True)\n",
        "    epsilon = 1e-5\n",
        "    inv = tf.rsqrt(variance + epsilon)\n",
        "    normalized = (input-mean)*inv\n",
        "    return scale*normalized + offset\n",
        "\n",
        "def safe_log(x, eps=1e-12):\n",
        "  return tf.log(x + eps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOuhjpfa4BYY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "#import * from ops\n",
        "#import utils\n",
        "\n",
        "class Net:\n",
        "  def __init__(self, name, n_filters, is_training, norm=None):\n",
        "    self.name = name\n",
        "    self.n_filters = n_filters\n",
        "    self.reuse = False\n",
        "    self.norm = norm\n",
        "    self.is_training = is_training\n",
        "\n",
        "  def __call__(self, input):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input: batch_size x width x height x n_channels\n",
        "    Returns:\n",
        "      output: same size as input\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(self.name):\n",
        "      # conv layers\n",
        "      conv1 = conv_layer(input, 3, self.n_filters, reuse=self.reuse, norm=self.norm, is_training=self.is_training, name='conv1')\n",
        "      conv2 = conv_layer(conv1, 3, self.n_filters, reuse=self.reuse, norm=self.norm, is_training=self.is_training, name='conv2')\n",
        "      conv3 = conv_layer(conv2, 3, self.n_filters, reuse=self.reuse, norm=self.norm, is_training=self.is_training, name='conv3')\n",
        "      conv4 = conv_layer(conv3, 3, self.n_filters, reuse=self.reuse, norm=self.norm, is_training=self.is_training, name='conv4')\n",
        "      \n",
        "      output = last_conv_layer(conv4, 3, 1, reuse=self.reuse, name='output')\n",
        "      \n",
        "    # set reuse=True for next call\n",
        "    self.reuse = True\n",
        "    self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def sample(self, input):\n",
        "    output = self.__call__(input)\n",
        "    #image = utils.batch_convert2int(self.__call__(input))\n",
        "    #image = tf.image.encode_jpeg(tf.squeeze(image, [0]))\n",
        "    return tf.squeeze(output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNq4k_0x4EFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "#import ops\n",
        "#import utils\n",
        "#from reader import Reader\n",
        "#from net import Net\n",
        "\n",
        "REAL_LABEL = 0.9\n",
        "\n",
        "class FusionCNN:\n",
        "  def __init__(self,\n",
        "               n_filters = 32,\n",
        "               batch_size=8,\n",
        "               norm=None,\n",
        "               learning_rate=2e-4,\n",
        "               beta1=0.5,\n",
        "              ):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      n_filters: integer, number of filters for each convolutional layer\n",
        "      batch_size: integer, batch size\n",
        "      norm: 'None, 'instance' or 'batch'\n",
        "      learning_rate: float, learning rate for Adam\n",
        "      beta1: float, momentum term of Adam\n",
        "    \"\"\"\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.n_filters = n_filters\n",
        "    self.norm = norm\n",
        "\n",
        "    self.is_training = tf.placeholder_with_default(True, shape=[], name='is_training')\n",
        "\n",
        "    self.net = Net('net', n_filters=self.n_filters, is_training=self.is_training, norm=self.norm)\n",
        "\n",
        "\n",
        "  def model(self, x, label):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: ensor of float, sample of the validation set\n",
        "      label: tensor of float, label related to \"x\"\n",
        "    Output:\n",
        "      loss: float, loss value for the current batch\n",
        "      fused: tensor of float, output of the CNN for the input \"x\"\n",
        "      :return: summaries_training: tf.summary for the current training step\n",
        "    \"\"\"\n",
        "  \n",
        "    fused = self.net(x)\n",
        "    loss = self.l2_loss(fused, label)\n",
        "\n",
        "    # summary\n",
        "    summaries_training_list = [\n",
        "                               tf.summary.scalar('training_error',loss)\n",
        "                              ]\n",
        "    summaries_training = tf.summary.merge(summaries_training_list)\n",
        "\n",
        "    return loss, fused, summaries_training\n",
        "\n",
        "  def validation_summary(self, x, label, mean_loss):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: tensor of float, sample of the validation set\n",
        "      label: tensor of float, label related to \"x\"\n",
        "      mean_loss: float, validation error in the current epoch\n",
        "    Output:\n",
        "      validation_summary: tf.summary, summury for the validation set, images for a sample of the set is saved\n",
        "    \"\"\"\n",
        "    fused = self.net(x)\n",
        "\n",
        "    x_shape = tf.shape(x)\n",
        "    x_tof = tf.slice(x, [0, 0, 0, 1], [1, x_shape[1], x_shape[2], 1])\n",
        "    x_stereo = tf.slice(x, [0, 0, 0, 0], [1, x_shape[1], x_shape[2], 1])\n",
        "    label_toShow = tf.slice(label, [0, 0, 0, 0], [1, x_shape[1], x_shape[2], 1])\n",
        "    fused_toShow = tf.slice(fused, [0, 0, 0, 0], [1, x_shape[1], x_shape[2], 1])\n",
        "    summaries_val_list = [tf.summary.image('val/input_tof', x_tof),\n",
        "                               tf.summary.image('val/input_stereo', x_stereo),\n",
        "                               tf.summary.image('val/label', label_toShow),\n",
        "                               tf.summary.image('val/fused', fused_toShow),\n",
        "\n",
        "                               tf.summary.scalar('validation_error', mean_loss)\n",
        "                               ]\n",
        "    summaries_val = tf.summary.merge(summaries_val_list)\n",
        "\n",
        "    return summaries_val\n",
        "\n",
        "  def optimize(self, loss):\n",
        "    def make_optimizer(loss, variables, name='Adam'):\n",
        "      \"\"\" Adam optimizer with the given learning rate [COMMENTED: for the first 100k steps (~100 epochs)\n",
        "          and a linearly decaying rate that goes to zero over the next 100k steps]\n",
        "      \"\"\"\n",
        "      \n",
        "      learning_rate = self.learning_rate\n",
        "      \n",
        "      beta1 = self.beta1\n",
        "      \n",
        "      global_step = tf.Variable(0, trainable=False)\n",
        "      \"\"\"\n",
        "\t  end_learning_rate = 0.0\n",
        "      start_decay_step = 100000\n",
        "      decay_steps = 100000\n",
        "\t  learning_rate = (\n",
        "          tf.where(\n",
        "                  tf.greater_equal(global_step, start_decay_step),\n",
        "                  tf.train.polynomial_decay(starter_learning_rate, global_step-start_decay_step,\n",
        "                                            decay_steps, end_learning_rate,\n",
        "                                            power=1.0),\n",
        "                  starter_learning_rate\n",
        "          )\n",
        "\n",
        "      )\"\"\"\n",
        "      #tf.summary.scalar('learning_rate/{}'.format(name), learning_rate)\n",
        "\n",
        "      learning_step = (\n",
        "          tf.train.AdamOptimizer(learning_rate, beta1=beta1, name=name)\n",
        "                  .minimize(loss, global_step=global_step, var_list=variables)\n",
        "      )\n",
        "      return learning_step\n",
        "\n",
        "    Net_optimizer = make_optimizer(loss, self.net.variables, name='Adam_net')\n",
        "\n",
        "    with tf.control_dependencies([Net_optimizer]):\n",
        "      return tf.no_op(name='optimizers')\n",
        "\n",
        "\n",
        "  def l2_loss(self,output, label):\n",
        "    \"\"\" loss (MSE norm)\n",
        "    \"\"\"\n",
        "    loss = tf.reduce_mean(tf.squared_difference(label,output))\n",
        "    return loss\n",
        "\t\n",
        "  def l1_loss(self,output, label):\n",
        "    \"\"\" loss (MAE norm)\n",
        "    \"\"\"\n",
        "    loss = tf.reduce_mean(tf.absolute_difference(label,output))\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hx5ZCWTl4WDt",
        "colab_type": "code",
        "outputId": "2336665c-3bce-42b1-b1e9-baf038d8a422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tables\n",
        "from datetime import datetime\n",
        "#from dataset import *\n",
        "#from model import FusionCNN \n",
        "from math import ceil\n",
        "\n",
        "\n",
        "# load the imdb file containing dataset\n",
        "print(\"loading dataset\")\n",
        "filepath = 'dataset.mat'\n",
        "mat_content = tables.open_file(filepath)\n",
        "\n",
        "training_data = mat_content.root.training_data[:]\n",
        "training_label = mat_content.root.training_label[:]\n",
        "validation_full_data = mat_content.root.validation_full_data[:]\n",
        "validation_full_label = mat_content.root.validation_full_label[:]\n",
        "test_data = mat_content.root.test_data[:]\n",
        "test_label = mat_content.root.test_label[:]\n",
        "\n",
        "print(\"training samples: {}\".format(len(training_data)))\n",
        "print(\"validation samples: {}\".format(len(validation_full_data)))\n",
        "print(\"test samples: {}\".format(len(test_data)))\n",
        "\n",
        "print(training_data.shape)\n",
        "print(training_label.shape)\n",
        "print(validation_full_data.shape)\n",
        "print(validation_full_label.shape)\n",
        "print(test_data.shape)\n",
        "print(test_label.shape)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading dataset\n",
            "training samples: 3902\n",
            "validation samples: 8\n",
            "test samples: 15\n",
            "(3902, 128, 128, 2)\n",
            "(3902, 128, 128, 1)\n",
            "(8, 500, 920, 2)\n",
            "(8, 500, 920, 1)\n",
            "(15, 500, 920, 2)\n",
            "(15, 500, 920, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2BC9LKEzmMvS",
        "colab_type": "code",
        "outputId": "f1a2b5b3-f8d8-40cd-8273-a5f77ff49534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "epoch_num = 100\n",
        "batch_size = 4\n",
        "learning_rate = 1e-5\n",
        "n_filters = 8\n",
        "early_stopping = True\n",
        "early_stopping_patience = 4 #number of epochs with no improvement after which training will be stopped.\n",
        "\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "checkpoints_dir = \"checkpoints/{}\".format(current_time)\n",
        "#--------------------------------------------------------------------------\n",
        "#------------------------------Graph Structure-----------------------------\n",
        "#--------------------------------------------------------------------------\n",
        "print(\"creating graph\")\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    x = tf.placeholder(tf.float32, shape=[None, None, None, 2]) #input images\n",
        "    y = tf.placeholder(tf.float32, shape=[None, None, None, 1]) #output fused disparity map\n",
        "    mean_val_loss = tf.placeholder(tf.float32)\n",
        "    \n",
        "    #initialize dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(buffer_size=2000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.repeat()\n",
        "\n",
        "    iter = dataset.make_initializable_iterator()\n",
        "    batch_features, batch_labels = iter.get_next()\n",
        "    \n",
        "    cnn = FusionCNN(\n",
        "                   n_filters = n_filters,\n",
        "                   batch_size=batch_size,\n",
        "                   learning_rate=learning_rate)\n",
        "\n",
        "    loss, fused, summaries_training = cnn.model(batch_features, batch_labels)\n",
        "\n",
        "    optimizer = cnn.optimize(loss)\n",
        "\n",
        "    summaries_val = cnn.validation_summary(batch_features, batch_labels, mean_val_loss)\n",
        "\n",
        "    # Add ops to save and restore all the variables.\n",
        "    saver = tf.train.Saver()\n",
        "    train_writer = tf.summary.FileWriter(checkpoints_dir, graph)\n",
        "    val_writer = tf.summary.FileWriter(checkpoints_dir, graph,filename_suffix='_Val')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating graph\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "634ee46c-dda9-49ae-850e-7272e92e4a70",
        "id": "xIRzeRVVTZxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "cell_type": "code",
      "source": [
        "file = open('trainingStat_learningRate_'+str(learning_rate)+'_batchSize_'+str(batch_size)+'.txt','a')\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "\n",
        "\n",
        "print(\"start training\")\n",
        "with tf.Session(graph=graph,config=config) as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  step_counter_glob = 0\n",
        "  best_val_mse = float(\"inf\")\n",
        "  stopping_epoch = 0\n",
        "  train_steps_per_epoch = ceil(len(training_data)/batch_size)\n",
        "  val_steps_per_epoch = ceil(len(validation_full_data)/batch_size)\n",
        "  \n",
        "  for epoch in range(epoch_num):\n",
        "      train_mse_acc = 0\n",
        "      \n",
        "      # initialise iterator with train data\n",
        "      sess.run(iter.initializer, feed_dict={x: training_data, y: training_label})\n",
        "\n",
        "      #start epoch\n",
        "      for step_train in range(train_steps_per_epoch):\n",
        "          _, loss_val, summary = sess.run([optimizer, loss, summaries_training])\n",
        "          train_mse_acc += loss_val\n",
        "          step_counter_glob += 1\n",
        "          train_writer.add_summary(summary, step_counter_glob)\n",
        "\n",
        "      # end of epoch: start validation\n",
        "      validation_mse_acc = 0\n",
        "      \n",
        "      # initialise iterator with val data\n",
        "      sess.run(iter.initializer, feed_dict={x: validation_full_data, y: validation_full_label})\n",
        "      for step_val in range(val_steps_per_epoch):\n",
        "          validation_mse_acc += loss.eval()\n",
        "      \n",
        "      #calculate mean squared error \n",
        "      validation_mse = validation_mse_acc/step_val\n",
        "      train_mse = train_mse_acc / step_train\n",
        "      message = 'epoch %d training_mse %g validation_mse %g step_counter %d \\n'%(epoch, train_mse, validation_mse, step_train)\n",
        "      print(message)\n",
        "      file.write(message)\n",
        "\n",
        "      # write summaries\n",
        "      [summaries_val_ev] = sess.run([summaries_val],feed_dict={mean_val_loss: validation_mse})\n",
        "      val_writer.add_summary(summaries_val_ev, epoch)\n",
        "      val_writer.flush()\n",
        "      train_writer.flush()\n",
        "      \n",
        "      #early stopping check\n",
        "      if (validation_mse < best_val_mse):\n",
        "        stopping_epoch = 0\n",
        "        best_val_mse = validation_mse\n",
        "      else:\n",
        "        stopping_epoch += 1\n",
        "      if early_stopping and stopping_epoch >= early_stopping_patience:\n",
        "        print(\"Early stopping is trigger at epoch: {}\".format(epoch))\n",
        "        break\n",
        "  \n",
        "  \n",
        "  # Save the variables to disk.\n",
        "  save_path = saver.save(sess, checkpoints_dir + \"/model.ckpt\")\n",
        "  print(\"Model saved in file: %s\" % save_path)\n",
        "  \n",
        "  # initialise iterator with val data\n",
        "  sess.run(iter.initializer, feed_dict={x: test_data, y: test_label})\n",
        "  test_mse_acc = 0\n",
        "  test_steps_per_epoch = ceil(len(test_data)/batch_size)\n",
        "  for step_test in range(test_steps_per_epoch):\n",
        "      test_mse_acc += loss.eval()\n",
        "  test_mse = test_mse_acc/step_test\n",
        "  file.write('test error %g \\n' % (test_mse))\n",
        "  file.close()\n",
        "  val_writer.close()\n",
        "  train_writer.close()\n",
        "  "
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training\n",
            "epoch 0 training_mse 1796.46 validation_mse 616.194 step_counter 975 \n",
            "\n",
            "epoch 1 training_mse 25.9466 validation_mse 74.9415 step_counter 975 \n",
            "\n",
            "epoch 2 training_mse 11.4036 validation_mse 71.2354 step_counter 975 \n",
            "\n",
            "epoch 3 training_mse 10.9456 validation_mse 66.5981 step_counter 975 \n",
            "\n",
            "epoch 4 training_mse 10.2746 validation_mse 57.7132 step_counter 975 \n",
            "\n",
            "epoch 5 training_mse 9.44942 validation_mse 50.6434 step_counter 975 \n",
            "\n",
            "epoch 6 training_mse 8.63151 validation_mse 40.7524 step_counter 975 \n",
            "\n",
            "epoch 7 training_mse 7.87929 validation_mse 31.7911 step_counter 975 \n",
            "\n",
            "epoch 8 training_mse 7.20991 validation_mse 26.8192 step_counter 975 \n",
            "\n",
            "epoch 9 training_mse 6.77567 validation_mse 22.087 step_counter 975 \n",
            "\n",
            "epoch 10 training_mse 6.5269 validation_mse 19.2209 step_counter 975 \n",
            "\n",
            "epoch 11 training_mse 6.3993 validation_mse 17.369 step_counter 975 \n",
            "\n",
            "epoch 12 training_mse 6.33182 validation_mse 16.593 step_counter 975 \n",
            "\n",
            "epoch 13 training_mse 6.29448 validation_mse 15.4748 step_counter 975 \n",
            "\n",
            "epoch 14 training_mse 6.26519 validation_mse 15.1643 step_counter 975 \n",
            "\n",
            "epoch 15 training_mse 6.24895 validation_mse 15.3672 step_counter 975 \n",
            "\n",
            "epoch 16 training_mse 6.26884 validation_mse 15.3664 step_counter 975 \n",
            "\n",
            "epoch 17 training_mse 6.21363 validation_mse 14.8539 step_counter 975 \n",
            "\n",
            "epoch 18 training_mse 6.20643 validation_mse 14.7159 step_counter 975 \n",
            "\n",
            "epoch 19 training_mse 6.18607 validation_mse 14.5839 step_counter 975 \n",
            "\n",
            "epoch 20 training_mse 6.16931 validation_mse 14.4296 step_counter 975 \n",
            "\n",
            "epoch 21 training_mse 6.19002 validation_mse 14.35 step_counter 975 \n",
            "\n",
            "epoch 22 training_mse 6.14197 validation_mse 14.4816 step_counter 975 \n",
            "\n",
            "epoch 23 training_mse 6.13287 validation_mse 14.0518 step_counter 975 \n",
            "\n",
            "epoch 24 training_mse 6.11244 validation_mse 13.9166 step_counter 975 \n",
            "\n",
            "epoch 25 training_mse 6.09833 validation_mse 13.9311 step_counter 975 \n",
            "\n",
            "epoch 26 training_mse 6.1011 validation_mse 13.9706 step_counter 975 \n",
            "\n",
            "epoch 27 training_mse 6.07147 validation_mse 14.1392 step_counter 975 \n",
            "\n",
            "epoch 28 training_mse 6.06501 validation_mse 14.5491 step_counter 975 \n",
            "\n",
            "Early stopping is trigger at epoch: 28\n",
            "Model saved in file: checkpoints/20190429-1358/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hnrD5GibFy5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard.notebook\n",
        "%tensorboard --logdir ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbs3koR2BRct",
        "colab_type": "code",
        "outputId": "43ca1370-4370-415c-a370-b5086d183537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#-----------------------------------------------------------------------\n",
        "#-----------------------------Show Results------------------------------\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "  validation_data_shape = validation_full_data.shape\n",
        "  n = validation_data_shape[0]\n",
        "  canvas_orig = np.empty((validation_data_shape[0], validation_data_shape[1]))\n",
        "  canvas_recon = np.empty((validation_data_shape[0], validation_data_shape[1]))\n",
        "  for i in range(n):\n",
        "        batch, batch_gt, _ = test_dataset.next_batch(1)#n\n",
        "        # compute CNN output\n",
        "        g = sess.run(fused, feed_dict={x_image: batch})\n",
        "\n",
        "        tof_gt = np.zeros((validation_data_shape[1], validation_data_shape[2]))\n",
        "        stereo_gt =  np.zeros((validation_data_shape[1], validation_data_shape[2]))\n",
        "        tof_est =  np.zeros((validation_data_shape[1], validation_data_shape[2]))\n",
        "        stereo_est = np.zeros((validation_data_shape[1], validation_data_shape[2]))\n",
        "        # Display GT data\n",
        "        tof_gt[:,:] = (batch_gt[0,:,:,0])\n",
        "        stereo_gt[:,:] = (batch_gt[0,:,:,1])\n",
        "        # Draw the reconstructed data\n",
        "        tof_est[:,:] = (g[0,:,:,0])\n",
        "        stereo_est[:,:] = (g[0,:,:,1])\n",
        "\n",
        "        print(\"ToF Results\")\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plot = plt.imshow(tof_gt, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\n",
        "        plt.colorbar()\n",
        "        plt.title(\" ToF Error GT\")\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(tof_est, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\n",
        "        plt.colorbar()\n",
        "        plt.title(\"ToF Estimated Error\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Stereo Results\")\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plot = plt.imshow(stereo_gt, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\n",
        "        plt.colorbar()\n",
        "        plt.title(\"Stereo Error GT\")\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(stereo_est, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\n",
        "        plt.colorbar()\n",
        "        plt.title(\"Stereo Estimated Error\")\n",
        "        plt.show()\n",
        "\"\"\""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#-----------------------------------------------------------------------\\n#-----------------------------Show Results------------------------------\\n#-----------------------------------------------------------------------\\n\\n\\n  validation_data_shape = validation_full_data.shape\\n  n = validation_data_shape[0]\\n  canvas_orig = np.empty((validation_data_shape[0], validation_data_shape[1]))\\n  canvas_recon = np.empty((validation_data_shape[0], validation_data_shape[1]))\\n  for i in range(n):\\n        batch, batch_gt, _ = test_dataset.next_batch(1)#n\\n        # compute CNN output\\n        g = sess.run(fused, feed_dict={x_image: batch})\\n\\n        tof_gt = np.zeros((validation_data_shape[1], validation_data_shape[2]))\\n        stereo_gt =  np.zeros((validation_data_shape[1], validation_data_shape[2]))\\n        tof_est =  np.zeros((validation_data_shape[1], validation_data_shape[2]))\\n        stereo_est = np.zeros((validation_data_shape[1], validation_data_shape[2]))\\n        # Display GT data\\n        tof_gt[:,:] = (batch_gt[0,:,:,0])\\n        stereo_gt[:,:] = (batch_gt[0,:,:,1])\\n        # Draw the reconstructed data\\n        tof_est[:,:] = (g[0,:,:,0])\\n        stereo_est[:,:] = (g[0,:,:,1])\\n\\n        print(\"ToF Results\")\\n        plt.figure(figsize=(10, 5))\\n        plt.subplot(121)\\n        plot = plt.imshow(tof_gt, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\\n        plt.colorbar()\\n        plt.title(\" ToF Error GT\")\\n\\n        plt.subplot(122)\\n        plt.imshow(tof_est, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\\n        plt.colorbar()\\n        plt.title(\"ToF Estimated Error\")\\n        plt.show()\\n\\n        print(\"Stereo Results\")\\n        plt.figure(figsize=(10, 5))\\n        plt.subplot(121)\\n        plot = plt.imshow(stereo_gt, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\\n        plt.colorbar()\\n        plt.title(\"Stereo Error GT\")\\n\\n        plt.subplot(122)\\n        plt.imshow(stereo_est, origin=\"upper\", cmap=\"jet\", clim=(-0.4, 0.4))\\n        plt.colorbar()\\n        plt.title(\"Stereo Estimated Error\")\\n        plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    }
  ]
}