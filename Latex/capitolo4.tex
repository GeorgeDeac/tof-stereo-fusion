\chapter{Analisi dei risultati}

\section{Training e validation}
Con l'architettura presentata e una volta pronto il codice necessario, è stato possibile procedere con il training della rete e con la successiva valutazione dei risultati.
Gli iperparametri dell'addestramento sono stati impostati nel seguente modo:
\begin{itemize}
    \item durata del training: 50 epoche
    \item learning rate: 0.001
    \item batch size: 32, il più alto possibile con la memoria GPU a disposizione.
    \item loss function: MSE
\end{itemize}
In figura \ref{training} e \ref{validation} sono mostrati i grafici degli andamenti del loss di training e validation.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\columnwidth]{final_training.png}
    \caption[Andamento del training loss]{Andamento del training loss}
    \label{training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\columnwidth]{final_validation.png}
    \caption[Andamento del validation loss]{Andamento del validation loss}
    \label{validation}
\end{figure}

\section{Test e risultati sperimentali}
Nelle figure seguenti sono mostrati alcuni risultati sperimentali della rete addestrata. La prima e la seconda colonna mostrano gli input ToF e Stereo rispettivamente. Si nota nei dati ToF la presenza di alcuni "buchi" sulle superfici poco riflettenti e sulle regioni affette dal "multipath error", ossia in prossimità delle zone di incidenza tra le superfici. Per quanto riguardo lo stereo, si notano in particolare accuratezze limitate sui bordi.\\
La terza colonna mostra la fusione, ossia l'output della rete. Infine nella quarta colonna vengono illustrati i ground truth. La fusione è in grado di estrarre le informazioni più accurate da entrambe le sorgenti, tappando le lacune del ToF ed eliminando gli artefatti lungo i bordi dello stereo. \\

\noindent\makebox[\linewidth][s]{\textbf{ | ToF Input | Stereo Input | Output Disparity | Ground Truth |}}\\
\includegraphics[width=1.0\columnwidth]{test_fig_1.png}
\includegraphics[width=1.0\columnwidth]{test_fig_2.png}
\includegraphics[width=1.0\columnwidth]{test_fig_3.png}
\includegraphics[width=1.0\columnwidth]{test_fig_4.png}

\section{Confronti con architetture differenti}
Confrontiamo ora i risultati che abbiamo ottenuto con quelli acquisiti da una semplice rete illustrata in figura \ref{simple_net}. Questa architettura non include blocchi residuali, normalizzazioni e convoluzioni dilatate.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{simple_net.png}
    \caption{Diagramma di una semplice CNN senza blocchi residuali, convoluzioni dilatate e normalizzazioni}
    \label{simple_net}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\columnwidth]{simplenet_vs_final.png}
    \caption[Confronto del validation loss della nostra architettura contro una versione più elementare]{
        Confronto del validation loss della nostra architettura contro una versione più elementare. \\
        \fcolorbox{black}{cyan}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad La nostra rete (figura \ref{my_net})\\
        \fcolorbox{black}{red}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad La CNN elementare (figura \ref{simple_net})
    }
    \label{simplevsfinal}
\end{figure}

Si nota immediatamente dalla figura \ref{simplevsfinal} che la nostra rete performa assai meglio della rete con architettura elementare convergendo molto più rapidamente e con meno oscillazioni.